/**
 * Static provider registry.
 * Each entry defines connection details for an LLM API provider.
 * The protocol field determines which streaming implementation to use.
 *
 * Local providers use requiresApiKey: false and serve the OpenAI-compatible
 * REST protocol on localhost. CORS must be enabled on the local server:
 *   Ollama:    OLLAMA_ORIGINS=http://localhost:5173 ollama serve
 *   LM Studio: App → Developer → enable CORS
 *   Jan.ai:    Settings → Advanced → CORS → add http://localhost:5173
 *   LocalAI:   CORS enabled by default
 */
export const PROVIDERS = {
  openrouter: {
    id: 'openrouter',
    name: 'OpenRouter',
    description: 'Access 200+ models through a unified API',
    keyPlaceholder: 'sk-or-...',
    keyUrl: 'https://openrouter.ai/keys',
    protocol: 'openai-compat',
    baseUrl: 'https://openrouter.ai/api/v1',
    completionsEndpoint: '/chat/completions',
    modelsEndpoint: '/models',
    defaultModel: 'anthropic/claude-sonnet-4-5-20250929',
    extraHeaders: () => ({ 'HTTP-Referer': window.location.origin }),
    supportsModelFetch: true,
    supportsStreamOptions: true,
  },
  openai: {
    id: 'openai',
    name: 'OpenAI',
    description: 'GPT-4o, GPT-4.1, o3, and more',
    keyPlaceholder: 'sk-...',
    keyUrl: 'https://platform.openai.com/api-keys',
    protocol: 'openai-compat',
    baseUrl: 'https://api.openai.com/v1',
    completionsEndpoint: '/chat/completions',
    modelsEndpoint: '/models',
    defaultModel: 'gpt-4o',
    extraHeaders: () => ({}),
    supportsModelFetch: true,
    supportsStreamOptions: true,
    modelFilter: (m) => /^(gpt-|o[1-9]|chatgpt-)/.test(m.id),
  },
  anthropic: {
    id: 'anthropic',
    name: 'Anthropic',
    description: 'Claude models via direct API',
    keyPlaceholder: 'sk-ant-...',
    keyUrl: 'https://console.anthropic.com/settings/keys',
    protocol: 'anthropic-native',
    baseUrl: 'https://api.anthropic.com/v1',
    completionsEndpoint: '/messages',
    modelsEndpoint: '/models',
    defaultModel: 'claude-sonnet-4-5-20250929',
    extraHeaders: () => ({}),
    supportsModelFetch: true,
    // Metadata for enriching fetched models (API doesn't return these)
    knownModelMeta: {
      'claude-opus-4-6':     { supportedParameters: ['temperature', 'tools', 'reasoning'], contextLength: 200000, maxCompletionTokens: 32768 },
      'claude-sonnet-4-5':   { supportedParameters: ['temperature', 'tools', 'reasoning'], contextLength: 200000, maxCompletionTokens: 16384 },
      'claude-opus-4':       { supportedParameters: ['temperature', 'tools', 'reasoning'], contextLength: 200000, maxCompletionTokens: 32768 },
      'claude-sonnet-4':     { supportedParameters: ['temperature', 'tools', 'reasoning'], contextLength: 200000, maxCompletionTokens: 16384 },
      'claude-haiku-4-5':    { supportedParameters: ['temperature', 'tools'], contextLength: 200000, maxCompletionTokens: 8192 },
      'claude-3-5-sonnet':   { supportedParameters: ['temperature', 'tools'], contextLength: 200000, maxCompletionTokens: 8192 },
      'claude-3-5-haiku':    { supportedParameters: ['temperature', 'tools'], contextLength: 200000, maxCompletionTokens: 8192 },
      'claude-3-opus':       { supportedParameters: ['temperature', 'tools'], contextLength: 200000, maxCompletionTokens: 4096 },
    },
    // Fallback if fetch fails
    hardcodedModels: [
      { id: 'claude-sonnet-4-5-20250929', name: 'Claude Sonnet 4.5', supportedParameters: ['temperature', 'tools', 'reasoning'], contextLength: 200000, maxCompletionTokens: 16384 },
      { id: 'claude-opus-4-20250514', name: 'Claude Opus 4', supportedParameters: ['temperature', 'tools', 'reasoning'], contextLength: 200000, maxCompletionTokens: 32768 },
      { id: 'claude-haiku-4-5-20251001', name: 'Claude Haiku 4.5', supportedParameters: ['temperature', 'tools'], contextLength: 200000, maxCompletionTokens: 8192 },
    ],
  },
  perplexity: {
    id: 'perplexity',
    name: 'Perplexity',
    description: 'Search-augmented AI models',
    keyPlaceholder: 'pplx-...',
    keyUrl: 'https://www.perplexity.ai/settings/api',
    protocol: 'openai-compat',
    baseUrl: 'https://api.perplexity.ai',
    completionsEndpoint: '/chat/completions',
    defaultModel: 'sonar-pro',
    extraHeaders: () => ({}),
    supportsModelFetch: false,
    hardcodedModels: [
      { id: 'sonar-pro', name: 'Sonar Pro', supportedParameters: ['temperature'], contextLength: 200000, maxCompletionTokens: 8192 },
      { id: 'sonar', name: 'Sonar', supportedParameters: ['temperature'], contextLength: 128000, maxCompletionTokens: 8192 },
    ],
  },

  // ── Local LLM Providers ────────────────────────────────────────────────────
  ollama: {
    id: 'ollama',
    name: 'Ollama (Local)',
    description: 'Locally-run models via Ollama',
    keyPlaceholder: '(no key required)',
    keyUrl: 'https://ollama.com',
    protocol: 'openai-compat',
    baseUrl: 'http://localhost:11434/v1',
    completionsEndpoint: '/chat/completions',
    modelsEndpoint: '/models',
    defaultModel: 'llama3.2',
    extraHeaders: () => ({}),
    supportsModelFetch: true,
    supportsStreamOptions: false,
    requiresApiKey: false,
    localSetup: 'Run: OLLAMA_ORIGINS=http://localhost:5173 ollama serve',
  },
  lmstudio: {
    id: 'lmstudio',
    name: 'LM Studio (Local)',
    description: 'Locally-run models via LM Studio',
    keyPlaceholder: 'lm-studio (any string)',
    keyUrl: 'https://lmstudio.ai',
    protocol: 'openai-compat',
    baseUrl: 'http://localhost:1234/v1',
    completionsEndpoint: '/chat/completions',
    modelsEndpoint: '/models',
    defaultModel: '',
    extraHeaders: () => ({}),
    supportsModelFetch: true,
    supportsStreamOptions: false,
    requiresApiKey: false,
    localSetup: 'Enable Local Server in LM Studio, then allow CORS in Developer settings',
  },
  janai: {
    id: 'janai',
    name: 'Jan.ai (Local)',
    description: 'Locally-run models via Jan.ai',
    keyPlaceholder: 'jan-ai (any string)',
    keyUrl: 'https://jan.ai',
    protocol: 'openai-compat',
    baseUrl: 'http://localhost:1337/v1',
    completionsEndpoint: '/chat/completions',
    modelsEndpoint: '/models',
    defaultModel: '',
    extraHeaders: () => ({}),
    supportsModelFetch: true,
    supportsStreamOptions: false,
    requiresApiKey: false,
    localSetup: 'In Jan: Settings → Advanced → CORS → add http://localhost:5173',
  },
  localai: {
    id: 'localai',
    name: 'LocalAI (Local)',
    description: 'Self-hosted multi-model server',
    keyPlaceholder: 'localai (any string)',
    keyUrl: 'https://localai.io',
    protocol: 'openai-compat',
    baseUrl: 'http://localhost:8080/v1',
    completionsEndpoint: '/chat/completions',
    modelsEndpoint: '/models',
    defaultModel: '',
    extraHeaders: () => ({}),
    supportsModelFetch: true,
    supportsStreamOptions: false,
    requiresApiKey: false,
    localSetup: 'LocalAI enables CORS by default. Run: docker run -p 8080:8080 localai/localai',
  },
};

export const PROVIDER_ORDER = ['openrouter', 'openai', 'anthropic', 'perplexity', 'ollama', 'lmstudio', 'janai', 'localai'];
